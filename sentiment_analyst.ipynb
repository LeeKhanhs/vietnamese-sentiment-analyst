{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Đọc file test "
      ],
      "metadata": {
        "id": "2ahoLhvBfzS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Read the CSV file\n",
        "data = pd.read_csv('Vongck.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MinBZGrfl4UJ",
        "outputId": "198aed24-c601-4896-de0d-8a1cc1cc0af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                     Tiến sĩ câu giờ, hoàng đế gây mê\\n\n",
              "1      Mới học 2 bữa thôi là đã nghiện cách giảng dạy...\n",
              "2      kêu giơ tay phát biểu cộng điểm nhưng có cái n...\n",
              "3               thầy mãi là number one trong lòng em !\\n\n",
              "4      Thầy như là người được chọn, sỡ hữu haki bá vư...\n",
              "                             ...                        \n",
              "495    THầy là người ru ngủ giỏi nhất em từng gặp <3 ...\n",
              "496    Cô rất dễ tính, cho bài hơi khó nhưng bù lại g...\n",
              "497                                  ông vua đọc slide\\n\n",
              "498                     Giọng giảng bài to rõ, dễ hiểu\\n\n",
              "499    Không quá nổi trội so với giáo viên trường mìn...\n",
              "Name: comment, Length: 500, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocesing\n"
      ],
      "metadata": {
        "id": "ucxjQ88m9Jts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atj3F4Sbg1Sb",
        "outputId": "49c99751-393e-4386-911f-db6178fedecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI1M-qpPhqEZ",
        "outputId": "80d8419c-ecac-4fb3-b817-46c765d36e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loại bỏ những từ không cần thiết"
      ],
      "metadata": {
        "id": "Gt4AHtwYf_Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import codecs\n",
        "# download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "def create_stopwordlist():\n",
        "    f = codecs.open('vietnamese_stopword.txt', encoding='utf-8')\n",
        "    datavn = []\n",
        "    null_data = []\n",
        "    for i, line in enumerate(f):\n",
        "        line = repr(line)\n",
        "        line = line[1:len(line)-3]\n",
        "        datavn.append(line)\n",
        "    return datavn\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "def remove_zw_words(tokens):\n",
        "    return [token for token in tokens if not re.search(r'[zw]', token)]\n",
        "column_name = 'comment'\n",
        "data[column_name] = data[column_name].astype(str)\n",
        "# lowercase\n",
        "data[column_name] = data[column_name].apply(lambda x: x.lower())\n",
        "\n",
        "# punctuation removal\n",
        "data[column_name] = data[column_name].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "# tokenization\n",
        "data[column_name] = data[column_name].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# stopword removal\n",
        "stop_words = create_stopwordlist()\n",
        "data[column_name] = data[column_name].apply(lambda x: [token for token in x if not token in stop_words])\n",
        "data[column_name] = data[column_name].apply(lambda x: [remove_numbers(token) for token in x])\n",
        "# remove words containing 'z' or 'w'\n",
        "data[column_name] = data[column_name].apply(remove_zw_words)\n",
        "# rejoin tokens into text\n",
        "data[column_name] = data[column_name].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# save the preprocessed data to a new CSV file\n",
        "# data.to_csv('preprocessed.csv', index=False)\n",
        "# data['comment']=data['comment'].apply(normalizer)\n",
        "# data.to_csv('processed_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbhQpaRf9MlM",
        "outputId": "18f7fe67-012f-41e1-d20e-e767aceb19c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chỉnh sửa chính tả "
      ],
      "metadata": {
        "id": "32v8FuwbgElh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from keras.models import load_model\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams,word_tokenize\n",
        "import numpy as np\n",
        "import re\n",
        "import unidecode\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sys\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "model = load_model(\"spell.h5\")\n",
        "model.make_predict_function()\n",
        "\n",
        "\n",
        "\n",
        "NGRAM=5  \n",
        "MAXLEN=40 \n",
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
        "accepted_char=list((string.digits + ''.join(letters)))\n",
        "\n",
        "def extract_phrases(text):\n",
        "    pattern = r'\\w[\\w ]*|\\s\\W+|\\W+'\n",
        "    return re.findall(pattern, text)\n",
        "\n",
        "def encoder_data(text, maxlen=MAXLEN):\n",
        "        text = \"\\x00\" + text\n",
        "        x = np.zeros((maxlen, len(alphabet)))\n",
        "        for i, c in enumerate(text[:maxlen]):\n",
        "            x[i, alphabet.index(c)] = 1\n",
        "        if i < maxlen - 1:\n",
        "          for j in range(i+1, maxlen):\n",
        "            x[j, 0] = 1\n",
        "        return x\n",
        "      \n",
        "def decoder_data(x):\n",
        "    x = x.argmax(axis=-1)\n",
        "    return ''.join(alphabet[i] for i in x)\n",
        "\n",
        "\n",
        "def nltk_ngrams(words, n=5):\n",
        "    return ngrams(words.split(), n)\n",
        "      \n",
        "def guess(ngram):\n",
        "    text = ' '.join(ngram)\n",
        "    preds = model.predict(np.array([encoder_data(text)]), verbose=0)\n",
        "    return decoder_data(preds[0]).strip('\\x00')\n",
        "\n",
        "def correct(sentence):\n",
        "    for i in sentence:\n",
        "        if i not in accepted_char:\n",
        "            sentence=sentence.replace(i,\" \")\n",
        "    ngrams = list(nltk_ngrams(sentence, n=NGRAM))\n",
        "    guessed_ngrams = list(guess(ngram) for ngram in ngrams)\n",
        "    candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n",
        "    for nid, ngram in (enumerate(guessed_ngrams)):\n",
        "        for wid, word in (enumerate(re.split(' +', ngram))):\n",
        "            candidates[nid + wid].update([word])\n",
        "    output = ' '.join(c.most_common(1)[0][0] for c in candidates)     \n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(data['comment'])):\n",
        "    try:\n",
        "        corrected = correct(data['comment'][i]) # áp dụng hàm correct vào câu hiện tại\n",
        "        data['comment'][i] = corrected # cập nhật phần tử hiện tại trong danh sách ban đầu bằng câu đã sửa chữa\n",
        "    except:\n",
        "        pass # nếu có lỗi xảy ra, bỏ qua câu hiện tại và tiếp tục vòng lặp\n"
      ],
      "metadata": {
        "id": "l-IWPjktghwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Model and Training#"
      ],
      "metadata": {
        "id": "-Efy87dFC0si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('trainLK.csv')"
      ],
      "metadata": {
        "id": "BT2QefQTIApQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = get_train_data('data_phase_1.txt')\n",
        "X_train, y_train = train_data['comment'], train_data['label']\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "vectorizer = CountVectorizer(max_features=10000)\n",
        "classifier = LinearSVC(C=1, class_weight='balanced')\n",
        "model_min = Pipeline(\n",
        "    [\n",
        "        (\"vectorizer\", vectorizer),\n",
        "        (\"classifier\", classifier),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_min.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "4WNC_d9r1WcD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "8c70156b-8c5c-41b4-c300-68bc9e541651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', CountVectorizer(max_features=10000)),\n",
              "                ('classifier', LinearSVC(C=1, class_weight='balanced'))])"
            ],
            "text/html": [
              "<style>#sk-container-id-25 {color: black;background-color: white;}#sk-container-id-25 pre{padding: 0;}#sk-container-id-25 div.sk-toggleable {background-color: white;}#sk-container-id-25 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-25 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-25 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-25 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-25 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-25 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-25 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-25 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-25 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-25 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-25 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-25 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-25 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-25 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-25 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-25 div.sk-item {position: relative;z-index: 1;}#sk-container-id-25 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-25 div.sk-item::before, #sk-container-id-25 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-25 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-25 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-25 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-25 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-25 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-25 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-25 div.sk-label-container {text-align: center;}#sk-container-id-25 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-25 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-25\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer(max_features=10000)),\n",
              "                (&#x27;classifier&#x27;, LinearSVC(C=1, class_weight=&#x27;balanced&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-73\" type=\"checkbox\" ><label for=\"sk-estimator-id-73\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer(max_features=10000)),\n",
              "                (&#x27;classifier&#x27;, LinearSVC(C=1, class_weight=&#x27;balanced&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-74\" type=\"checkbox\" ><label for=\"sk-estimator-id-74\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=10000)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-75\" type=\"checkbox\" ><label for=\"sk-estimator-id-75\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=1, class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RRESULT#"
      ],
      "metadata": {
        "id": "EteHCq4MC8jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = data['comment'], data['label']\n",
        "y_predict = model_min.predict(X_test)"
      ],
      "metadata": {
        "id": "SdaZAMYzmlJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "precision = precision_score(y_pred=y_predict, y_true=y_test, average='weighted')\n",
        "recall = recall_score(y_pred=y_predict, y_true=y_test, average='weighted')\n",
        "f1 = f1_score(y_pred=y_predict, y_true=y_test, average='weighted')\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "pQAnKO2pkl-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'submit.txt'\n",
        "with open(filename, 'w') as f:\n",
        "    for item in y_predict:\n",
        "        f.write(\"%s\\n\" % item)\n"
      ],
      "metadata": {
        "id": "4Flws18jQ57_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}